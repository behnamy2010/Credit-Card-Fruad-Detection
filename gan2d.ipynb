{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gRu9kpkYyztz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as metrique\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,GRU, Dense, Embedding, Dropout,Input, Attention, Layer, Concatenate, Permute, Dot, Multiply, Flatten\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from tensorflow.keras import backend as K, regularizers, Model, metrics\n",
    "from tensorflow.keras.backend import cast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= pd.read_csv('fetureselectedcreditcard.csv', na_filter=True)\n",
    "# y=pd.read_csv('Target.csv', na_filter=True)\n",
    "# X_train,X_test,y_train,y_test = train_test_split(x,y, test_size=0.2,shuffle=False)\n",
    "\n",
    "# X_train.to_csv('ur_X_train.csv',index=False)\n",
    "# X_test.to_csv('ur_X_test.csv',index=False)\n",
    "\n",
    "# y_train.to_csv('ur_y_train.csv',index=False)\n",
    "# y_test.to_csv('ur_y_test.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('ur_X_train.csv', na_filter=True)\n",
    "y_train=pd.read_csv('ur_y_train.csv',na_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4uohF3BPMIU",
    "outputId": "b1d24e82-da51-42ca-f9ed-0d021c2538ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 18)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OaLUwRhPPhW",
    "outputId": "170cccab-ded4-4110-e701-3d103902a590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56962, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0,X_train1=np.array_split(X_train,n)\n",
    "y_train0,y_train1=np.array_split(y_train,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((113923, 18), (113922, 18))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train0.shape,X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((113923, 1), (113922, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train0.shape,y_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, reuse=None):\n",
    "    with tf.variable_scope(\"gen\", reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(inputs=z, units=128)\n",
    "        alpha = 0.01\n",
    "        hidden1 = tf.maximum(alpha*hidden1, hidden1)\n",
    "        hidden2 = tf.layers.dense(inputs=hidden1, units=64)\n",
    "        hidden2 = tf.maximum(alpha*hidden2, hidden2)\n",
    "        output = tf.layers.dense(hidden2, units=18, activation=tf.nn.tanh)\n",
    "        return output\n",
    "\n",
    "def discriminator(X, reuse=None):\n",
    "    with tf.variable_scope(\"dis\", reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(inputs=X, units=128)\n",
    "        alpha = 0.01\n",
    "        hidden1 = tf.maximum(alpha*hidden1, hidden1)\n",
    "        hidden2 = tf.layers.dense(inputs=hidden1, units=128)\n",
    "        hidden2 = tf.maximum(alpha*hidden2, hidden2)\n",
    "        logits = tf.layers.dense(hidden2, units=1)\n",
    "        output = tf.sigmoid(logits)\n",
    "        return output, logits\n",
    "\n",
    "real_data = tf.placeholder(tf.float32, shape=[None, 18])\n",
    "z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G = generator(z)\n",
    "\n",
    "D_output_real, D_logits_real = discriminator(real_data)\n",
    "D_output_fake, D_logits_fake  = discriminator(G, reuse=True)\n",
    "\n",
    "# LOSS\n",
    "def loss_func(logits_in, labels_in):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in, labels=labels_in))\n",
    "\n",
    "D_real_loss = loss_func(D_logits_real, tf.ones_like(D_logits_real)*0.9)\n",
    "D_fake_loss = loss_func(D_logits_fake, tf.zeros_like(D_logits_real))\n",
    "\n",
    "D_loss = D_real_loss + D_fake_loss\n",
    "G_loss = loss_func(D_logits_fake, tf.ones_like(D_logits_fake))\n",
    "\n",
    "learning_rate = 0.001\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in tvars if 'dis' in var.name]\n",
    "g_vars = [var for var in tvars if 'gen' in var.name]\n",
    "\n",
    "D_trainer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(D_loss, var_list=d_vars)\n",
    "G_trainer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(G_loss, var_list=g_vars)\n",
    "\n",
    "def next_batch(num, data):\n",
    "    '''\n",
    "    Return a total of `num` random samples. \n",
    "    '''\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data.iloc[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN(X_train,y_train,no_of_samples_to_generate):\n",
    "    X_ones = X_train[y_train.Class == 1]\n",
    "    X_ones.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    batch_size = 10\n",
    "    epochs = 5000\n",
    "    init = tf.global_variables_initializer()\n",
    "    samples = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            num_batches = len(X_ones) // batch_size\n",
    "            for i in range(num_batches):\n",
    "\n",
    "                batch_X = next_batch(batch_size, X_ones)            \n",
    "                batch_z = np.random.uniform(-1,1, size=(batch_size,100))\n",
    "\n",
    "                _ = sess.run(D_trainer, feed_dict={real_data:batch_X, z: batch_z})\n",
    "                _ = sess.run(G_trainer, feed_dict={z: batch_z})         \n",
    "\n",
    "        sample_z = np.random.uniform(-1,1,size=(no_of_samples_to_generate, 100)) # new samples will be generated\n",
    "        gen_sample = sess.run(generator(z, reuse=True), feed_dict={z:sample_z})\n",
    "        samples.append(gen_sample)\n",
    "    X_fake = pd.DataFrame(samples[0], columns=X_train.columns)\n",
    "    X_augmented_data = np.append(X_train, np.array(X_fake), axis=0)\n",
    "    y_fake = np.ones(no_of_samples_to_generate)\n",
    "    y_fake=y_fake.reshape(-1,1)\n",
    "    y_augmented_data = np.append(y_train, y_fake, axis=0)\n",
    "    return X_augmented_data,y_augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k0=y_train0.value_counts()[1]\n",
    "k1=y_train1.value_counts()[1]\n",
    "X_train0,y_train0= GAN(X_train0,y_train0,no_of_samples_to_generate=k0)\n",
    "X_train1,y_train1= GAN(X_train1,y_train1,no_of_samples_to_generate=k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train0),type(y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114164, 18), (114098, 18))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train0.shape,X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0=pd.DataFrame(X_train0, columns=X_train.columns)\n",
    "X_train1=pd.DataFrame(X_train1, columns=X_train.columns)\n",
    "y_train0=pd.DataFrame(y_train0, columns=y_train.columns)\n",
    "y_train1=pd.DataFrame(y_train1, columns=y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0.to_csv('ur_X_train0.csv',index=False)\n",
    "X_train1.to_csv('ur_X_train1.csv',index=False)\n",
    "y_train0.to_csv('ur_y_train0.csv',index=False)\n",
    "y_train1.to_csv('ur_y_train1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
